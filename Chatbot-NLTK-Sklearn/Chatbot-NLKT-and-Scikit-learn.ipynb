{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Chatbot with NLTK and Scikit-learn\n",
    "\n",
    "Let's create a chatbot with Python's NLTK library and Scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP?\n",
    "NLP is a way for computers to analyze, understand, and derive meaning from human language in a smart and useful way. By utilizing NLP, developers can organize and structure knowledge to perform tasks such as automatic summarization, translation, named entity recognition, relationship extraction, sentiment analysis, speech recognition, and topic segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import io\n",
    "import random\n",
    "import string # to process standard python strings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading and installing NLTK\n",
    "NLTK(Natural Language Toolkit) is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries.\n",
    "\n",
    "[Natural Language Processing with Python](http://www.nltk.org/book/) provides an guide to do programming for language processing.\n",
    "If this is your first time to use NLKT, you'll need to check... [Installing NLTK Data](https://www.nltk.org/data.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing NLTK Packages\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('popular', quiet=True)\n",
    "#nltk.download('punkt') first-time use only\n",
    "#nltk.download('wordnet') first-time use only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the corpus\n",
    "\n",
    "We use the Wikipedia page for chatbots as our corpus. Copy the contents from the page and place it in a text file named ‘chatbot.txt’. However, you can use any corpus of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('chatbot.txt',errors = 'ignore') \n",
    "raw = f.read()\n",
    "raw = raw.lower() # convert the text to lowercase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We need to convert the text format (string) data into umerical vector so that computer can perform Machine learning task. Therefore, before we start with any NLP project we need to pre-process it to make it ideal for working. Basic text pre-processing includes:\n",
    "\n",
    "**1. Tokenization**\n",
    "* Converting the entire text into **uppercase** or **lowercase**, so that the algorithm does not treat the same words in different cases as different\n",
    "\n",
    "* **Tokenization**: Tokenization is just the term used to describe the process of converting the normal text strings into a list of tokens i.e words that we actually want. Sentence tokenizer can be used to find the list of sentences and Word tokenizer can be used to find the list of words in strings.\n",
    "\n",
    "\n",
    "NLTK data package includes a pre-trained tokenizer for English.\n",
    "\n",
    "**2. Noise Removal**\n",
    "* Removing **Noise** i.e everything that isn’t in a standard number or letter.\n",
    "* Removing the **Stop words**. Sometimes, some extremely common words which would appear to be of little value in helping select documents matching a user need are excluded from the vocabulary entirely. These words are called stop words.\n",
    "\n",
    "\n",
    "**3. Normalization**\n",
    "* **Stemming**: Stemming is the process of reducing inflected (or sometimes derived) words to their stem, base or root form — generally a written word form. Example, if we were to stem the following words: “Stems”, “Stemming”, “Stemmed”, “and Stemtization”, the result would be a single word “stem”.\n",
    "\n",
    "Example of Stemming\n",
    "_<br>Form---->Suffix—-> Stem_\n",
    "<br>studies—>   -es  —->studi\n",
    "<br>studying—> -ing —->study\n",
    "\n",
    "* **Lemmatization**: Lemmatization is a slight variant of stemming. The major difference between these is, that, stemming can often create non-existent words, whereas lemmas are actual words. So, your root stem, meaning the word you end up with, is not something you can just look up in a dictionary, but you can look up a lemma. Examples of Lemmatization are that “run” is a base form for words like “running” or “ran” or that the word “better” and “good” are in the same lemma so they are considered the same.\n",
    "\n",
    "Example of Lemmatization\n",
    "_<br>Form--------> Morphological info————————> Lemma_\n",
    "<br>studies—>   Third person, present form of verb —->study\n",
    "<br>studying—> Gerund of the verb ---------------------—->study\n",
    "\n",
    "\n",
    "\n",
    "Which is better?\n",
    "<br>We can say developing a stemmer is far simpler than building a lemmatizer. Lemmatization requires more computational resource and linguistic knowledge to create the dictionaries that allow the algorithm to look for the proper form of the word. \n",
    "We have seen the benefits of a lemmatizer for search engines, but there are more applications of lemmatization, like textual bases or e-commerce search.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_tokens = nltk.sent_tokenize(raw) # convert to list of sentences \n",
    "listword_tokens = nltk.word_tokenize(raw) # convert to list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "# WordNet is a semantically-oriented dictionary of English included in NLTK.\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greeting and Keyword matching\n",
    "\n",
    "Next, we define a function for a greeting from the bot. If a user’s input matches with one of the greeting words, the bot shall return a greeting response. [ELIZA](https://en.wikipedia.org/wiki/ELIZA) uses a simple keyword matching for greetings. We use the same concept here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GREETING_INPUTS = (\"hello\", \"hi\", \"greetings\",\"hey\")\n",
    "GREETING_RESPONSES = [\"Hello, Please input your question.\", \"Hi! type your question please.\"]\n",
    "def greeting(sentence):\n",
    "    for word in sentence.split():\n",
    "        if word.lower() in GREETING_INPUTS:\n",
    "            return random.choice(GREETING_RESPONSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words\n",
    "After the preprocessing, we need to transform text into a meaningful vector (or array) of numbers in order to analyze the text and run algorithms.\n",
    "<br>The bag-of-words is a representation of text that describes the occurrence of words within a document. It involves two things:\n",
    "\n",
    "* A vocabulary of known words.\n",
    "\n",
    "* A measure of the presence of known words.\n",
    "\n",
    "Why is it is called a “bag” of words? \n",
    "<br>That is because any information about the order or structure of words in the document is discarded and the model is only **concerned with whether the known words occur in the document, not where they occur in the document.**\n",
    "\n",
    "The intuition behind the Bag of Words is that documents are similar if they have similar content. Also, we can learn something about the meaning of the document from its content alone.\n",
    "\n",
    "For example, if our dictionary contains the words {Learning, is, the, not, great}, and we want to vectorize the text “Learning is great”, we would have the following vector: (1, 1, 0, 0, 1).\n",
    "\n",
    "\n",
    "<img src=\"BOW.jpg\" style=\"width: 90%/\">\n",
    "\n",
    "source: [From text to vectors with BoW and TF-IDF](https://maelfabien.github.io/machinelearning/NLP_2)\n",
    "\n",
    "\n",
    "\n",
    "The number in the matrix are simply the count of the tokens in each document. This is called the Term Frequency (TF) approach.\n",
    "<br>However, this approach is not popular anymore due to the limitations and newly emerged approach. The logic of TF approach is the more frequent a word, the more importance we attach to it within each document. However, this can be problematic since common words, like cat or dog in our example, do not bring much information about the document it refers to. In other words, words that appear the most are not the most interesting to extract information from a document. Plus, we could leverage the fact that the words that appear rarely bring a lot of information within the document."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Approach\n",
    "A problem with the Bag of Words approach is that highly frequent words start to get larger score (i.e. just because a word is frequently appeared, it does not mean the word is important in the document), and it may not contain much informational content. Also, it will give more weight to longer documents than shorter documents.\n",
    "\n",
    "\n",
    "One approach is to rescale the frequency of words by how often they appear through the documents so that the scores for frequent words like “the” which is also frequent across all documents are penalized. \n",
    "<br>Instead of filling the BOW matrix with the raw counting, how about a scoring with the term frequency multiplied by the inverse document frequency? It is intended to reflect how important a word is to a document in a collection or corpus. This approach to scoring is called Term Frequency-Inverse Document Frequency (TF-IDF) where:\n",
    "\n",
    "\n",
    "\n",
    "**Term Frequency: is a scoring of the frequency of the word in the current document.**\n",
    "\n",
    "```\n",
    "TF = (Number of times term t appears in a document)/(Number of terms in the document)\n",
    "```\n",
    "\n",
    "**Inverse Document Frequency: is a scoring of how rare the word is across documents.**\n",
    "\n",
    "```\n",
    "IDF = 1+log(N/n), where, N is the number of documents and n is the number of documents a term t has appeared in.\n",
    "```\n",
    "\n",
    "<br>\n",
    "For exaple, we have:\n",
    "\n",
    "* Document1 = The sky is blue\n",
    "\n",
    "* Document2 = The sky is not blue\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"TFIDF.png\" style=\"width: 50%/\">\n",
    "\n",
    "\n",
    "The only word which differenciates the two document is 'not' and it is important from TF-IDF perspective (i.e. gets larger score)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity\n",
    "\n",
    "TF-IDF weight is a weight often used in information retrieval and text mining. This weight is a statistical measure used to evaluate how important a word is to a document in a collection or corpus\n",
    "\n",
    "```\n",
    "Cosine Similarity (d1, d2) =  Dot product(d1, d2) / ||d1|| * ||d2||\n",
    "```\n",
    "where d1,d2 are two non zero vectors.\n",
    "\n",
    "\n",
    "<img src=\"CosineSimilarity.png\" style=\"width: 70%/\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "To generate a response from our bot for input questions, the concept of document similarity will be used. We define a function response which searches the user’s utterance for one or more known keywords and returns one of several possible responses. If it doesn’t find the input matching any of the keywords, it returns a response:” I am sorry, I don’t understand you”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def response(user_response):\n",
    "    robo_response = ''\n",
    "    \n",
    "    # take user response and add it to sentence_tokens\n",
    "    sentence_tokens.append(user_response) \n",
    "    \n",
    "    # calculate tf-idf scores within the user responce and corpus/doc\n",
    "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
    "    \n",
    "    # generate word counts for the words in the corpus\n",
    "    tfidf = TfidfVec.fit_transform(sentence_tokens)\n",
    "   \n",
    "    # calculate the similarity between user responce and the corpus\n",
    "    # 'tfidf[-1]' is user responce, tfidf is the corpus\n",
    "    vals_cosine = cosine_similarity(tfidf[-1], tfidf)\n",
    "    \n",
    "    # sort the cosine similarity to remove the user response appended in the corpus \n",
    "    # get the second largest element’s index from it’s 0 th row\n",
    "    # most large cosine similarity can be user input so we get the second largest element\n",
    "    idx = vals_cosine.argsort()[0][-2]\n",
    "    \n",
    "    # flatten() function gets a copy of an given array, and\n",
    "    # converts the 2d array into a 1d array (e.g. [[]] -> []\n",
    "    flat = vals_cosine.flatten()\n",
    "    \n",
    "    # then sort the cosine similarity \n",
    "    flat.sort() \n",
    "    \n",
    "    # if the input does't find any match, say \"I am sorry\"\n",
    "    # most large cosine similarity can be user input so we use the second largest element,\n",
    "    # to chech the match between the given input and corpus\n",
    "    req_tfidf = flat[-2] \n",
    "    if(req_tfidf == 0):\n",
    "        robo_response = robo_response + \"I am sorry, I don't understand you.\"\n",
    "        return robo_response\n",
    "    \n",
    "    # if the input find a match, retuern a sentence from the corpus\n",
    "    else:\n",
    "        robo_response = robo_response + sentence_tokens[idx]\n",
    "        return robo_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will feed the lines that we want our bot to say while starting and ending a conversation depending upon user’s input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ROBO: My name is Robo. I will answer your queries about Chatbots. If you want to exit, type 'Bye'\")\n",
    "while True:\n",
    "    user_response = input()\n",
    "    user_response = user_response.lower()\n",
    "    if user_response == 'bye':\n",
    "        print(\"ROBO: See you.\")\n",
    "        break\n",
    "    elif user_response == 'thanks' or user_response == 'thank you' :\n",
    "        print(\"ROBO: You're welcome.\")\n",
    "    elif greeting(user_response) != None:\n",
    "        print(\"ROBO: \" + greeting(user_response))\n",
    "    else:\n",
    "        print(\"ROBO: \", end = \"\")\n",
    "        print(response(user_response))\n",
    "        sentence_tokens.remove(user_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
