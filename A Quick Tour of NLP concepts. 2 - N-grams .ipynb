{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigrams, Bigrams, Trigrams, and N-grams "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N-gram is a sequence of N words. Let's take a look at the following examples.\n",
    "\n",
    "\n",
    "New Delhi => (is a 2-gram)\n",
    "\n",
    "The Three Musketeers => (is a 3-gram)\n",
    "\n",
    "She stood up slowly => (is a 4-gram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we assign a probability to the occurrence of an N-gram or the probability of a word occurring next in a sequence of words, it can be very useful. Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, it can help in deciding which N-grams can be chunked together to form single entities (e.g. “New Delhi” chunked together as one word, “high school” being chunked as one word).\n",
    "\n",
    "It can also help make next word predictions. Let' say you have the partial sentence “Please hand over your”. Then it is more likely that the next word is going to be “test” or “assignment” or “paper” than the next word being “school”.\n",
    "\n",
    "It can also help to make spelling error corrections. For instance, the sentence “drink cofee” could be corrected to “drink coffee” if you knew that the word “coffee” had a high probability of occurrence after the word “drink” and also the overlap of letters between “cofee” and “coffee” is high.\n",
    "As you can see, Assigning these probabilities has a huge potential in the NLP domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we understand this concept, we can build with it: that’s the N-gram model. Basically, an N-gram model predicts the occurrence of a word based on the occurrence of its N – 1 previous words. \n",
    "\n",
    "So here, we are answering the question – how far back in the history of a sequence of words should we go to predict the next word? For example, a bigram model (N = 2) predicts the occurrence of a word given only its previous word (as N – 1 = 1) in this case. Similarly, a trigram model (N = 3) predicts the occurrence of a word based on its previous two words (as N – 1 = 2) in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see a way to assign a probability to a word occurring next in a sequence of words. First, we need a large sample of English sentences (corpus)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say our corpus contains the following sentences:\n",
    "\n",
    "- He said thank you.\n",
    "- He said bye-bye as he walked through the door.\n",
    "- He went to Chennai.\n",
    "- New Delhi has nice weather.\n",
    "- It is raining in New Castle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s assume a bigram model. So we are going to find the probability of a word based only on its previous word. In general, we can say that this probability is (the number of times the previous word ‘wp’ occurs before the word ‘wn’) / (the total number of times the previous word ‘wp’ occurs in the corpus) = (Count (wp wn))/(Count (wp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the probability of the word “you” following the word “thank”, we can write this as P (you | thank) which is a conditional probability.\n",
    "This becomes equal to:\n",
    "\n",
    "=(No. of times “Thank You” occurs) / (No. of times “Thank” occurs) \n",
    "= 1/1 \n",
    "= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can say that whenever “Thank” occurs, it will be followed by “You” (This is because we have trained on a set of only five sentences and “Thank” occurred only once in the context of “Thank You”). Let’s see an example of a case when the preceding word occurs in different contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s calculate the probability of the word “Delhi” coming after “New”. We want to find the P (Delhi | New). This means that we are trying to find the probability that the next word will be “Delhi” given the word “New”. We can do this by:\n",
    "\n",
    "=(No of times “New Delhi” occurs) / (No. of times “New” occurs) \n",
    "= 2/3 \n",
    "= 0.67"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is because in our corpus, one of the three preceding “New” is followed by “Castle”. So, the P (Castle | New) = 1 / 3.\n",
    "In our corpus, only “Delhi” and “Castle” occur after “San” with the probabilities 2 / 3 and 1 / 3 respectively. So if we want to create a next word prediction software based on our corpus, and a user types in “San”, we will give two options: “Delhi” ranked most likely and “Castle” ranked less likely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen, N-grams are fixed-length ( n ) consecutive token sequences occurring in the text. A bigram has two tokens, a unigram one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reference\n",
    "Rao, Delip,McMahan, Brian. Natural Language Processing with PyTorch. O'Reilly Media.\n",
    "\n",
    "An Introduction to N-grams: What Are They and Why Do We Need Them? https://blog.xrds.acm.org/2017/10/introduction-n-grams-need/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
